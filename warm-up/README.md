# MAFS5440 Assignment 1

The Home Credit Default Risk dataset on the Kaggle is subjected as final project of the bootcamp, and I have spent a period of three weeks on this project personally and also with my sub-team members. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one 0.8034). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer/float dtype conversion on my datasets.

After a while I recognize that some Kagglers had already shared their distinctly combined blending performances on a bunch of submitted results from their different type of models. Thanks to their inspiration about this approach, which is completely associate to boosting strategy: "weaker predictors turns to be much stronger one". Therefore, I titled this study as "an Empirical Blend Boosting" **(a quite unique naming :)**. Here I would like to share you my boosted blending performance based on my +20 results having better than 0.802 AUC score and other +30 results having AUC score between 0.802 and 0.800. However, I do not used yet the second set of my submission results because just ~25 of them is enough to achieve the best result on the Kaggle :), and also it is a quite boring stuff to try possible linear combinations. On the other hand, I can aggressively enhance my AUC score, but I could not convince myself yet that blending is a meaningful thing for data science philosophy.

Mostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. I am also developing a micro model having less than 200 features with a 0.800 AUC score (as a future mission for me). Soon I will share my micro model on the Kaggle, there should be no problem about RAM usage :-).